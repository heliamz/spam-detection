{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f99361",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib tensorflow nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5fa724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5171 entries, 0 to 5170\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  5171 non-null   int64 \n",
      " 1   label       5171 non-null   object\n",
      " 2   text        5171 non-null   object\n",
      " 3   label_num   5171 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 161.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "spam    0.5\n",
       "ham     0.5\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a study notebook\n",
    "import pandas as pd\n",
    "SEED = 42\n",
    "df = pd.read_csv(\"Emails.csv\") # dowloaded from -< https://media.geeksforgeeks.org/wp-content/uploads/20250320162008521713/spam_ham_dataset.csv\n",
    "df.head()\n",
    "df.info()\n",
    "df[\"label\"].value_counts(normalize=True)\n",
    "\n",
    "spam_df = df[df[\"label\"] == \"spam\"]\n",
    "ham_df = df[df[\"label\"] == \"ham\"]\n",
    "\n",
    "# downsample ham to size of spam\n",
    "ham_sampled = ham_df.sample(n=len(spam_df),random_state=SEED)\n",
    "\n",
    "balanced_df = pd.concat([spam_df, ham_sampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "balanced_df[\"label\"].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bccb98c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\helia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: affordable lev ! itra &amp; v ( iagra ! o...</td>\n",
       "      <td>affordable lev itra v iagra overnight delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: urgent news alert ! ( otcbb : gspm ) ...</td>\n",
       "      <td>urgent news alert otcbb gspm gold hot stock pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: quality medications available with di...</td>\n",
       "      <td>quality medications available discounts prices...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 98 - 0432\\r\\ncan you please extend si...</td>\n",
       "      <td>98 0432 please extend sitara deal 156657 3 1 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: we have all your favorite programs at...</td>\n",
       "      <td>favorite programs incredibly low prices window...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Subject: affordable lev ! itra & v ( iagra ! o...   \n",
       "1  Subject: urgent news alert ! ( otcbb : gspm ) ...   \n",
       "2  Subject: quality medications available with di...   \n",
       "3  Subject: 98 - 0432\\r\\ncan you please extend si...   \n",
       "4  Subject: we have all your favorite programs at...   \n",
       "\n",
       "                                          clean_text  \n",
       "0     affordable lev itra v iagra overnight delivery  \n",
       "1  urgent news alert otcbb gspm gold hot stock pr...  \n",
       "2  quality medications available discounts prices...  \n",
       "3  98 0432 please extend sitara deal 156657 3 1 0...  \n",
       "4  favorite programs incredibly low prices window...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords') # like \"the\" \"a\" \"for\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # lower\n",
    "    text = text.lower()\n",
    "    # remove \"subject:\" prefix if present\n",
    "    if text.startswith(\"subject:\"):\n",
    "        text = text[len(\"subject:\"):]\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # tokenize by spaces\n",
    "    tokens = text.split()\n",
    "    # remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "balanced_df[\"clean_text\"] = balanced_df[\"text\"].astype(str).apply(preprocess_text)\n",
    "balanced_df[[\"text\", \"clean_text\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb9f0170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = balanced_df[\"clean_text\"].values\n",
    "y = balanced_df[\"label\"].map({\"ham\": 0, \"spam\": 1}).values  # binary labels\n",
    "\n",
    "# train+val vs test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec4382f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_words = 10000   # vocabulary size\n",
    "max_len = 100       # sequence length (number of tokens per email)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)  # fit only on training data\n",
    "\n",
    "def texts_to_padded(texts):\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(seqs, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "X_train_seq = texts_to_padded(X_train)\n",
    "X_test_seq  = texts_to_padded(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "739bef7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\helia\\OneDrive\\Desktop\\projects\\ai-ml\\spam-detector-tf\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "embedding_dim = 32\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "    layers.LSTM(16),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")  # probability of spam\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",       \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95db00da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "75/75 - 3s - 44ms/step - accuracy: 0.5321 - loss: 0.6912 - val_accuracy: 0.5717 - val_loss: 0.6824 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "75/75 - 1s - 19ms/step - accuracy: 0.8532 - loss: 0.3944 - val_accuracy: 0.9200 - val_loss: 0.2879 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "75/75 - 1s - 19ms/step - accuracy: 0.9525 - loss: 0.1957 - val_accuracy: 0.9433 - val_loss: 0.2237 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "75/75 - 1s - 19ms/step - accuracy: 0.9666 - loss: 0.1475 - val_accuracy: 0.9383 - val_loss: 0.2459 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "75/75 - 1s - 19ms/step - accuracy: 0.9704 - loss: 0.1316 - val_accuracy: 0.9467 - val_loss: 0.2152 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "75/75 - 1s - 19ms/step - accuracy: 0.9771 - loss: 0.1057 - val_accuracy: 0.9533 - val_loss: 0.1989 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "75/75 - 1s - 20ms/step - accuracy: 0.9779 - loss: 0.1040 - val_accuracy: 0.9433 - val_loss: 0.2113 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "75/75 - 1s - 19ms/step - accuracy: 0.9800 - loss: 0.0973 - val_accuracy: 0.9683 - val_loss: 0.1462 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "75/75 - 2s - 20ms/step - accuracy: 0.9829 - loss: 0.0869 - val_accuracy: 0.9617 - val_loss: 0.1642 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "75/75 - 1s - 20ms/step - accuracy: 0.9837 - loss: 0.0811 - val_accuracy: 0.9633 - val_loss: 0.1649 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "75/75 - 2s - 21ms/step - accuracy: 0.9854 - loss: 0.0736 - val_accuracy: 0.9650 - val_loss: 0.1603 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    min_lr=1e-5\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_seq, y_test),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01bd497f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9683 - loss: 0.1462\n",
      "Test Loss : 0.14618441462516785\n",
      "Test Accuracy : 0.9683333039283752\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_seq, y_test)\n",
    "print('Test Loss :',test_loss)\n",
    "print('Test Accuracy :',test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a625882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "[[291   9]\n",
      " [ 10 290]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      0.97      0.97       300\n",
      "        spam       0.97      0.97      0.97       300\n",
      "\n",
      "    accuracy                           0.97       600\n",
      "   macro avg       0.97      0.97      0.97       600\n",
      "weighted avg       0.97      0.97      0.97       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "y_test_pred_prob = model.predict(X_test_seq).ravel()\n",
    "y_test_pred = (y_test_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred, target_names=[\"ham\", \"spam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "102d0833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "0.9724323749542236\n"
     ]
    }
   ],
   "source": [
    "def predict_email(text):\n",
    "    # preprocess and convert to a batch of one\n",
    "    clean_text = preprocess_text(text)\n",
    "    seq = texts_to_padded([clean_text])  \n",
    "    prob_spam = model.predict(seq)[0, 0]\n",
    "    return float(prob_spam)\n",
    "\n",
    "example = \"Congratulations! You have won a $1000 Walmart gift card. Click here to claim the prize now.\"\n",
    "print(predict_email(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23399a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
